# Forecasting Lifting Accuracy with Machine Learning
========================================================

# Overview
This project analyzes data generated by six participants lifting weights while wearing a personal tracking device.  We process, analyze, select and combine 159 potential covariates to build a machine learning model using the random forest method that correctly predicts 100% of 20 cases using a subset of the covariates.  

The data for this project was developed by:
http://groupware.les.inf.puc-rio.br/har

The training data for this project is available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data for this project is available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

# Load libraries
```{r, echo=FALSE, message=FALSE, warning=FALSE}
lib <- "C:/Users/huntsrob/R/win-library/3.0"
libPaths <- .libPaths( c("C:/Users/huntsrob/R/win-library/3.0",.libPaths()))
setwd("//MSAD/ROOT/NA/NY/USERS/huntsrob/Desktop/R Review/Machine Learning")    
trainLoc <- "//MSAD/ROOT/NA/NY/USERS/huntsrob/Desktop/R Review/Machine Learning/pml-training.csv"

library(msversion)
addpkg("corrplot","0.73")
addpkg("randomForest","4.6-10")
addpkg("htmltools","0.3")
addpkg("dplyr", "0.4.0")
addpkg("timeSeries", "3022.101")
addpkg("timeDate", "3012.100")
addpkg("ggplot2","1.0.1")
addpkg("lubridate","1.5.0")
addpkg("lubridate","1.3.3")
addpkg("DBI","0.3.1")
addpkg("lazyeval","0.1.10")
addpkg("lattice","0.20-33-r30")
addpkg("data.table","1.9.6-r30")
addpkg("plyr","1.8.1")
addpkg("zoo", "1.7-12")
addpkg("gdata","2.17.0")
addpkg("gtools","3.1.0")
addpkg("readxl","0.1.0")
addpkg("psych","1.3.2")
addpkg("rmarkdown","0.8")
addpkg("reshape","0.8.4")
addpkg("caret","6.0-47")
addpkg("e1071","1.6-5")
addpkg("rpart","core")
addpkg("rpart","4.1-8")
addpkg("gbm","2.1")
addpkg("forecast","5.8")
addpkg("devtools","1.3")
addpkg("caret","6.0-62")
addpkg("foreach","1.4.1")
addpkg("iterators","1.0.6")
addpkg("car","2.0-25")
addpkg("pbkrtest","0.4-2")
addpkg("lme4","1.1-10")
addpkg("fracdiff","1.4-2")
addpkg("tseries","0.10-34")
addpkg("quadprog","1.5-5")
addpkg("modeltools","0.2-21")
addpkg("Matrix","1.2-3")
addpkg("minqa","1.2.1")
addpkg("nloptr","1.0.4")
addpkg("quantreg","5.05")
addpkg("SparseM","1.03")
addpkg("randomForest","4.6-10")
addpkg("lars","1.2")
addpkg("nls2","0.2")
addpkg("proto","0.3-10")
addpkg("corrgram","1.5")
addpkg("seriation","1.0-12")
addpkg("TSP","1.0-9")
addpkg("gclus","1.3.1")

install.packages("BAT",repos=NULL,type="source")
install.packages("glmnet",repos=NULL,type="source")
install.packages("AppliedPredictiveModeling",repos=NULL,type="source")
install.packages("CORElearn",repos=NULL,type="source")
install.packages("pgmm",repos=NULL,type="source")
install.packages("RANN",repos=NULL,type="source")
                      
library(msversion)
library(klaR)
library(base)
library(gclus)
library(TSP)
library(seriation)
library(corrgram)
library(RANN)
library(proto)
library(nls2)
library(lars)
library(randomForest)
library(SparseM)
library(Matrix)
library(modeltools)
library(quadprog)
library(tseries)
library(fracdiff)
library(lme4)
library(pbkrtest)
library(car)
library(iterators)
library(devtools)
library(foreach)
library(forecast)
library(gbm)
library(rpart)
library(e1071)
library(caret)
library(timeSeries)
library(timeDate)
library(ggplot2)
library(lubridate)
library(plyr)
library(dplyr)
library(DBI)
library(lazyeval)
library(lattice)
library(data.table)
library(zoo)
library(gtools)
library(gdata)
library(readxl)
library(htmltools)
library(psych)
library(reshape)
library(rmarkdown)
library(ElemStatLearn)
library(pgmm)
library(minqa)
library(nloptr)
library(quantreg)
library(SparseM)
library(ElemStatLearn)
library(caret)
library(CORElearn)
library(AppliedPredictiveModeling)
library(glmnet)
library(BAT)
library(forecast)
library(randomForest)
library(corrplot)
```


# Testing and Training Data Sets
We begin by loading the pml-training data set and examining its structure.  The structure of the training data set shows that many of the variables are in the incorrect format (e.g. numeric variables are factors and time variables aren't properly formatted).   

```{r}
dat = read.csv(trainLoc,header=TRUE)
str(dat)
```

We next divide the training data set into training and testing subsets, using 80% of the initial training data set for the sub-train data set.
```{r}
inTrain <- createDataPartition(y=dat$classe,p=0.8,list=FALSE)
training = dat[inTrain,]
testing = dat[-inTrain,]
```

# Preprocessing Functions
We developed a set of preprocessing functions for converting the columns of the training data to their correct formats and for removing variables from the dataset with zero variance; zero variance variables do not explain the variance of the outcome variable, classe, and so add noise.  The main preprocessing function calls a number of other functions, each designed to achieve the purpose described by its name.

```{r}
preProcessData <- function(dataset) {
  # Convert columns that should be numeric to numeric
  dataset <- convertFactorsToNumeric(dataset)
  dataset <- convertToNumeric(dataset)
  # Convert covariates that should be in date/time format to date/time format
  dataset <- convertToDateTime(dataset)
  # Create dummy variables from user_name, new_window
  dataset <- removeZeroVar(dataset)
  return(dataset)
}
```

The first function converts factor variables that should be numeric to numeric variables:
```{r}
convertFactorsToNumeric <- function(dataset) {
  # Convert factor vectors that should be numeric to numeric
  factor_col <- lapply(dataset,class) == "factor"
  factor_col_names <- array(names(dataset[,factor_col==TRUE]))
  factor_col <- which(colnames(dataset) %in% factor_col_names)
  nFactorCols <- nrow(factor_col_names)
  factor_col <- factor_col[-c(1,2,3,nFactorCols)] # Don't convert factor variables that are categorical
  dataset[,factor_col] <- lapply(dataset[,factor_col],as.character)
  dataset[,factor_col] <- lapply(dataset[,factor_col],as.numeric)
  return(dataset)
}
```

The second function converts all other variables types that should be numeric to numeric:
```{r}
convertToNumeric <- function(dataset) {
  # Convert all columns that should be numeric and that are not factors to numeric
  lastCol <- ncol(dataset)
  nonNumericCols <- c(1,2,3,4,5,6,7,lastCol)
  dataset[,-nonNumericCols] <- lapply(dataset[,-nonNumericCols],as.numeric)
  return(dataset)
}
```

The third function converts date/time fields to their appropriate format:
```{r}
convertToDateTime <- function(dataset) {
  dataset$cvtd_timestamp <- dmy_hm(dataset$cvtd_timestamp)
  return(dataset) 
}
```

The fourth function removes variables with zero variation:
```{r}
removeZeroVar <- function(dataset) {
nsv <- nearZeroVar(dataset,saveMetrics=TRUE)
dataset <- dataset[,nsv$nzv==FALSE]
return(dataset)
}
```

Functions 5-7 calculate the percent of each column in a data frame that consists of NA's so that covariates with a high percentage of NA's can be removed.
```{r}

percentNA <- function(dataset) {
  naPercent <- naCount(dataset)/colLength(dataset)
}

naCount <- function(x) sapply(x, function(y) sum(is.na(y)))
colLength <- function(x) sapply(x, function(y) length(y))
```

# Preprocessing
The core set of code does three things: preprocesses the training data, analyzes the covariates and builds a prediction model.

The following line of code preprocess the data using the functions previously loaded:
```{r}
training <- preProcessData(training)
```

```{r fig.width=7, fig.height=6}
ggp <- ggplot(training,aes(x=classe))
ggp + geom_histogram(fill="lightgreen")+ggtitle("Training Classe Distribution") + labs(x="Classe",y="Count of Classe")
```

Remove the first six variables from the training data set since they are descriptive and add little to no predictive value.
```{r}
training <- training[,-c(1:6)]
```

Remove variables with a majority of NA values.
```{r}
percentNA <- percentNA(training)
majorityNA <- names(percentNA[percentNA>.5])
training <- training[,!names(training) %in% majorityNA]
```

# Correlation Analysis
Calculate correlations among covariates.
```{r}
predictors <- training[,-ncol(training)]
corMatrix <- abs(cor(predictors))
diag(corMatrix) <- 0
```

The following chart shows that many of the variables are highly correlated and can be removed.  We remove correlated covariates because they explain the same behavior and are duplicative:
```{r fig.width=7, fig.height=6}
corrplot(corMatrix,type="upper",method="color")
```

Remove correlated covariates:
```{r}
highlyCor <- findCorrelation(corMatrix,0.7)
training <- training[,-highlyCor]
```

# Most Significant Variable Analysis
Create a random forest model to find the most important covariates
```{r}
set.seed(1234)
rfFit <- randomForest(classe~.,data=training,importance=TRUE,ntree=100)
```

The variable fits show that much of the variation, whether explained by the decrease in accuracy or the Gini coefficient, is explained by the top 10-15 variables:
```{r fig.width=7, fig.height=6}
varImpPlot(rfFit)
```

# Model Development
We now train two different machine learning models, Linear Discriminant Analysis (LDA) and Random Forest (RF) on the Training data set using PCA to preprocess the data (Note: I saved modrf and am reloading it for purposes of saving time rather than rerunning the model.  The code to train the model is modrf <- train(classe~.,data=training,method="rf",preProcess="pca"))
```{r}
modlda <- train(classe ~ ., data=training,method="lda",preProcess="pca")
modrf <- readRDS("rfModel.RDS")
```


# Prediction
Using the models developed in the prior step, we predict the outcomes of the testing data set using the Linear Discriminant and Random Forest models:

```{r}
plda <- predict(modlda,testing)
prf <- predict(modrf,testing)
```

# Model Accuracy
Finally, we assess the accuracy of the two models using confusion matrices, which compare predictions versus actuals.  The Random Forest model has a 99.62% prediction accuracy while the LDA model has only a 52.2% accuracy.
```{r}
confusionMatrix(plda,testing$classe)
confusionMatrix(prf,testing$classe)
```

The following table shows the close relationship between the predicted and actual values of classe in the training set using the Random Forest model with PCA preprocessing.
```{r fig.width=7, fig.height=6}
table(prf,testing$classe)
```